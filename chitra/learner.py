# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_learner.ipynb (unless otherwise specified).

__all__ = ['create_classifier', 'Learner']

# Cell
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.models import Model
from .datagenerator import Dataset

from typeguard import check_argument_types, check_return_type

# Cell
from PIL import Image
import numpy as np
from tf_keras_vis.utils import normalize
from tf_keras_vis.gradcam import GradcamPlusPlus, Gradcam
from functools import partial

import matplotlib.pyplot as plt
import matplotlib.cm as cm

# Cell
def create_classifier(base_model_fn:callable, num_classes:int,
                      weights='imagenet', dropout=0,
                      include_top=False,
                      name=None):

    outputs = 1 if num_classes == 2 else num_classes

    base_model = base_model_fn(
        include_top=include_top,
        weights=weights,
    )
    if include_top: return base_model
    model = tf.keras.Sequential(name=name)
    model.add(base_model)
    model.add(tf.keras.layers.GlobalAveragePooling2D())
    model.add(tf.keras.layers.Dropout(dropout))
    model.add(tf.keras.layers.Dense(outputs, name='output'))

    return model

# Cell
class Learner(Model):
    _AUTOTUNE = tf.data.experimental.AUTOTUNE

    def __init__(self, ds: Dataset, base_model_fn:callable, pretrained:bool=True, include_top=False, **kwargs):
        assert check_argument_types()

        super(Learner, self).__init__()
        self.ds = ds
        self.total = len(ds)
        self.NUM_CLASSES = ds.NUM_CLASSES
        self.gradcam = None
        self.include_top = include_top

        weights = 'imagenet' if pretrained else None

        #self.base_model = base_model_fn.name

        self.model = create_classifier(
            base_model_fn,
            self.NUM_CLASSES,
            dropout=kwargs.get('dropout', 0.5),
            include_top=include_top,
            name=kwargs.get('name', None)
        )


    def build(self): pass

    def summary(self): return self.model.summary()

    #def get_layer(name=None, index=None): return self.model(name, index)

    def compile(self, *args, **kwargs): return self.model.compile(*args, **kwargs)

    def call(self, *args, **kwargs): return self.model.call(*args, **kwargs)

    def fit(self, *args, **kwargs): return self.model.fit(*args, **kwargs)

    def warmup(self):pass

    def prewhiten(self, image):
        image = tf.cast(image, tf.float32)
        image = image / 127.5 - 1.0
        return image

    def rescale(self, image, label):
        image = self.prewhiten(image)
        return image, label

    def _get_optimizer(self,
                       optimizer,
                       momentum=0.9,
                       **kwargs
                      ):
        if optimizer.__name__=='SGD':
            optimizer = partial(optimizer,
                momentum=momentum,
                nesterov=kwargs.get('nesterov', True)
            )
        else:
            optimizer = partial(optimizer,
                momentum=momentum,
            )
        return optimizer


    def _prepare_dl(self, bs=8, shuffle=True):
        dl = ds.get_tf_dataset(shuffle=shuffle)
        dl = dl.map(self.rescale, Learner._AUTOTUNE)
        return dl.batch(bs).prefetch(Learner._AUTOTUNE)


    def cyclic_fit(self,
                   epochs,
                   batch_size,
                   lr_range=(1e-4, 1e-2),
                   optimizer=tf.keras.optimizers.SGD,
                   momentum=0.9,
                   validation_data=None,
                   callbacks=None,
                   *args,
                   **kwargs
                  ):

        self.max_lr, self.min_lr = lr_range

        step_size = 2 * len(self.ds)//batch_size

        lr_schedule = tfa.optimizers.Triangular2CyclicalLearningRate(
                                    initial_learning_rate=lr_range[0],
                                    maximal_learning_rate=lr_range[1],
                                    step_size=kwargs.get('step_size', step_size),
                                    scale_mode=kwargs.get('scale', 'cycle'))


        optimizer = self._get_optimizer(optimizer, momentum=momentum)
        optimizer = optimizer(learning_rate=lr_schedule)
        self.model.optimizer = optimizer

        return self.model.fit(
            self._prepare_dl(batch_size, kwargs.get('shuffle', True)),
            validation_data=validation_data,
            epochs=epochs,
            callbacks=callbacks
        )

    def gradcam(self, image:Image.Image, image_size=None):
        # assert check_argument_types()

        def model_modifier(m):
            """Converts sigmoid to linear
            """
            m.layers[-1].activation = tf.keras.activations.linear
            return m


        if image_size:
            image_size = self.ds.img_sz_list.get_size()
        image = image.resize(image_size)

        X = np.asarray(image, np.float32)
        X = self.prewhiten(X)
        X = np.expand_dims(X, 0)

        if self.gradcam is None:
            self.gradcam = Gradcam(self.model,
                              model_modifier,
                              clone=True)

        cam = gradcam(get_loss,
              X,
              penultimate_layer=-1, # model.layers number
              seek_penultimate_conv_layer=False,

             )

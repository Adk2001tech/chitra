{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datagenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datagenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "from chitra.image import read_image, resize_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator\n",
    "## components\n",
    "> components are methods that can be easily overridden\n",
    "- image path gen\n",
    "- image label gen\n",
    "- image resizer\n",
    "\n",
    "\n",
    "> the generator object will also support  callbacks that can update the components \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_filenames(root_dir):\n",
    "        root_dir = pathlib.Path(root_dir)\n",
    "        return glob(str(root_dir/'*'))\n",
    "    \n",
    "def get_label(filename):\n",
    "    return filename.split('/')[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ImageSizeList():\n",
    "    def __init__(self, img_sz_list=[]):\n",
    "        \n",
    "        if type(img_sz_list) in (list, tuple):\n",
    "            if not type(img_sz_list[0]) in (list, tuple):\n",
    "                img_sz_list = [img_sz_list]\n",
    "        \n",
    "        self.start_size = None\n",
    "        self.last_size = None\n",
    "        self.curr_size = None\n",
    "        self.img_sz_list = img_sz_list\n",
    "        \n",
    "        try:\n",
    "            self.start_size = img_sz_list[0]\n",
    "            self.last_size = img_sz_list[-1]\n",
    "            self.curr_size = img_sz_list[0]\n",
    "        except (IndexError, TypeError) as e:\n",
    "            print('No item present in the image size list')\n",
    "            self.curr_size = None # no item present in the list\n",
    "        \n",
    "          \n",
    "    def get_size(self):\n",
    "        img_sz_list = self.img_sz_list\n",
    "        try:\n",
    "            self.curr_size = img_sz_list.pop(0)\n",
    "        except (IndexError, AttributeError) as e:\n",
    "            print(f'Returning the last set size which is: {self.curr_size}')\n",
    "        \n",
    "        return self.curr_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No item present in the image size list\n",
      "Returning the last set size which is: None\n"
     ]
    }
   ],
   "source": [
    "img_sz_list = ImageSizeList(None)\n",
    "img_sz_list.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LabelEncoder():\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(self.labels)}\n",
    "        \n",
    "    def encode(self, label):\n",
    "        return self.label_to_idx[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    MAPPINGS = {\n",
    "        'PY_TO_TF': {str:tf.string, int:tf.int32, float:tf.float32},\n",
    "        \n",
    "        }\n",
    "    \n",
    "    def __init__(self, root_dir, image_size=None, transforms=None, label_encoder=None):\n",
    "        self.get_filenames = get_filenames\n",
    "        self.read_image = read_image\n",
    "        self.get_label = get_label\n",
    "        self.label_encoder = label_encoder\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.filenames = self.get_filenames(root_dir)\n",
    "        self.num_files = len(self.filenames)\n",
    "        self.img_sz_list= ImageSizeList(image_size)\n",
    "        \n",
    "        \n",
    "    def __len__(self): return len(self.filenames)\n",
    "    \n",
    "    \n",
    "    def _process(self, filename):\n",
    "        image = self.read_image(filename)\n",
    "        label = self.get_label(filename)\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "    def _reload(self):\n",
    "        self.filenames  = self.get_filenames(self.root_dir)\n",
    "        self.num_files = len(self.filenames)\n",
    "        \n",
    "    def _capture_return_types(self):\n",
    "        return_types = []\n",
    "        for e in self.generator():\n",
    "            outputs = e\n",
    "            break\n",
    "        if isinstance(outputs, tuple):\n",
    "            for ret_type in outputs:\n",
    "                return_types.append(\n",
    "                    ret_type.dtype if tf.is_tensor(ret_type) else Dataset.MAPPINGS['PY_TO_TF'][type(ret_type)]\n",
    "                )\n",
    "        else:\n",
    "            return_types.append(\n",
    "                ret_type.dtype if tf.is_tensor(ret_type) else Dataset.MAPPINGS['PY_TO_TF'][type(ret_type)]\n",
    "            )\n",
    "        return tuple(return_types)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        return self._process(filename)\n",
    "    \n",
    "    def update_component(self, component_name, new_component, reload=True):\n",
    "        setattr(self, component_name, new_component)\n",
    "        print(f'{component_name} updated with {new_component}')\n",
    "        self._reload()\n",
    "        \n",
    "    \n",
    "    def generator(self,):\n",
    "        img_sz = self.img_sz_list.get_size()\n",
    "        n = len(self.filenames)\n",
    "        for i in range(n):\n",
    "            image, label = self.__getitem__(i)\n",
    "            if img_sz: image = resize_image(image, img_sz)\n",
    "            if self.transforms: image = self.transforms(image)\n",
    "            yield image, label\n",
    "    \n",
    "    \n",
    "    def get_tf_dataset(self, output_shape=None):\n",
    "        datagen = tf.data.Dataset.from_generator(\n",
    "            self.generator,\n",
    "            self._capture_return_types(),\n",
    "            output_shape\n",
    "\n",
    "        )\n",
    "        return datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset('/data/aniket/tiny-imagenet/data/tiny-imagenet-200/train', image_size=[(96,96), (64,64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    return glob(f'{path}/*/images/*')\n",
    "\n",
    "def get_label(path):\n",
    "    return path.split('/')[-3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_filenames updated with <function load_files at 0x7faffca3e3b0>\n",
      "get_label updated with <function get_label at 0x7faff0ca7560>\n"
     ]
    }
   ],
   "source": [
    "ds.update_component('get_filenames', load_files)\n",
    "ds.update_component('get_label', get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'> n03584254\n"
     ]
    }
   ],
   "source": [
    "for e in ds.generator():\n",
    "    print(e[0].dtype, e[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_dataset(ds):\n",
    "    datagen = tf.data.Dataset.from_generator(\n",
    "                    ds.generator,\n",
    "                    ds._capture_return_types()\n",
    "    )\n",
    "    return datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning the last set size which is: (64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.string)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
